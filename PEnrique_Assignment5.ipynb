{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XIbQr92cFTrM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics \n",
        "from sklearn.naive_bayes import GaussianNB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roGhxr1VFb5f",
        "outputId": "eff4edbc-9544-43a5-867c-9cf843b7a4fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read in dataframe, clean data \n",
        "df = pd.read_csv('/content/drive/MyDrive/AAI510/MODULE1/home-credit-default-risk/application_train.csv')\n",
        "df = df.dropna()\n",
        "\n",
        "for column in df:\n",
        "  if df[column].dtypes == 'object':\n",
        "    df[column] = df[column].astype('category')\n",
        "    df[column] = df[column].cat.codes\n",
        "\n",
        "print('Shape of the dataframe: ', df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRDdUywsXdxT",
        "outputId": "886b4e95-152d-4c66-a06f-42e7ba20366b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the dataframe:  (8602, 122)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#seperate the targets from the data and complete train/test split \n",
        "y = df['TARGET']\n",
        "x = df.drop(columns=['TARGET']) \n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "hFL8xZMlXqTM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#perform GBT as the non Bayesian model to compare \n",
        "gbc = GradientBoostingClassifier(learning_rate=1.0, max_depth=3, random_state=0)\n",
        "gbc = gbc.fit(X_train, y_train)\n",
        "\n",
        "pred_train = gbc.predict(X_train)\n",
        "pred_test = gbc.predict(X_test)\n",
        "\n",
        "print(\"Train Accuracy:\", round(metrics.accuracy_score(y_train, pred_train),3))\n",
        "print(\"Test Accuracy:\", round(metrics.accuracy_score(y_test, pred_test),3))\n",
        "print(\"F1 Score:\", round(metrics.f1_score(pred_test, y_test, average=\"weighted\"),3))\n",
        "print(\"Recall Score:\", round(metrics.recall_score(pred_test, y_test, average=\"weighted\"),3))\n",
        "print(\"Precision Score:\", round(metrics.precision_score(pred_test, y_test, average=\"weighted\"),3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeQvkZN-38zh",
        "outputId": "1b18cc71-9c1a-4c4d-a316-038d4e97d915"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.978\n",
            "Test Accuracy: 0.91\n",
            "F1 Score: 0.914\n",
            "Recall Score: 0.91\n",
            "Precision Score: 0.917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#perform naive Bayesian model and return performance metrics \n",
        "nbc = GaussianNB()\n",
        "nbc = nbc.fit(X_train, y_train) \n",
        "\n",
        "pred_train  = nbc.predict(X_train) \n",
        "pred_test = nbc.predict(X_test) \n",
        "\n",
        "print(\"Train Accuracy:\", round(metrics.accuracy_score(y_train, pred_train),3))\n",
        "print(\"Test Accuracy:\", round(metrics.accuracy_score(y_test, pred_test),3))\n",
        "print(\"F1 Score:\", round(metrics.f1_score(pred_test, y_test, average=\"weighted\"),3))\n",
        "print(\"Recall Score:\", round(metrics.recall_score(pred_test, y_test, average=\"weighted\"),3))\n",
        "print(\"Precision Score:\", round(metrics.precision_score(pred_test, y_test, average=\"weighted\"),3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHUNSRV10v4i",
        "outputId": "6fe25ad0-b617-45e6-e858-36e20f8b42f2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.937\n",
            "Test Accuracy: 0.945\n",
            "F1 Score: 0.972\n",
            "Recall Score: 0.945\n",
            "Precision Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Naive Bayesian technique created a model that has slightly better performance across the board, except for the training accuracy, when compared to the gradient boosting classifier. These results are due to the Naive Bayes classifier's scalability, not being sensitive to irrelevant features, and not requiring as much training data to achieve good results. Although neither model appears to be overfitting based on the accuracies reported, the Naive Bayesian technique appears to do a better job of creating a model that can handle data that has not yet been seen. This can be attributed to the classifier's assumption that the elements of the feature vector are conditionally independent of each other. This simplification is not always true which is why the results are comparable to the gradient boosting classifier and only slightly outperforms it. "
      ],
      "metadata": {
        "id": "8fNmSrsJ-URi"
      }
    }
  ]
}